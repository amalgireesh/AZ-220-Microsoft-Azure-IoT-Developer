# Visualize a Data Stream in Power BI

> **IMPORTANT**: This lab has several service prerequisites that are not related to the Azure subscription you were given for the course:
>
> 1. The ability to sign in to a "Work or School Account" (Azure Active Directory account)
> 2. You must know your account sign-in name, which may not match your e-mail address.
> 3. Access to Power BI, which could be through:
>       1. An existing Power BI account
>       2. The ability to sign up for Power BI - some organizations block this.
>
> The first lab exercise will validate your ability to access Power BI.  If you are not successful in the first exercise, you will not be able to complete the lab, as there is no quick workaround for blocked access to a work or school account.

## Lab Scenario

You have developed a simulated IoT device that generates vibration data and other telemetry outputs that are representative of the conveyor belt system used in Contoso's cheese packaging process. You have built and tested a logging route that sends data to Azure Blob storage. You will now start work on a new route within IoT hub that will send telemetry data to an Azure Event Hubs service.

The primary difference between Azure IoT Hub and Azure Event Hubs is that Event Hubs is designed for big data streaming, while IoT hub is optimized for an IoT solution. Both services support ingestion of data with low latency and high reliability. Since Azure Event Hubs provides an input to Stream Analytics in a manner that is similar to IoT hub, your choice of Event Hubs in this case allows you explore an additional Azure service option within your solution.

### Make a Call to a Built-in Machine Learning Model

In this lab, you will be calling a built-in Machine Learning (ML) function named `AnomalyDetection_SpikeAndDip`. The `AnomalyDetection_SpikeAndDip` function uses a sliding window to analyze data for anomalies. The sliding window could be, for example, the most recent two minutes of telemetry data. The window advances in near real-time with the flow of telemetry. Generally speaking, if the size of the sliding window is increased to include more data, the accuracy of anomaly detection will increase as well (however, the latency also increases, so a balance must be found).

The function establishes a "normal" range for the data and then uses it to identify anomalies and assign a rating. It works like this: as the function continues to monitor the flow of data, the algorithm establishes a normal range of values, then compares new values against those norms. The result is a score for each value, expressed as a percentage that determines the confidence level that the given value is anomalous. As you may expect, low confidence levels can be ignored, but you may wonder what percentage confidence value is acceptable. In your query, you will set this tipping point at 95%.

There are always complications, like when there are gaps in the data (the conveyor belt stops for a while, perhaps). The algorithm handles voids in the data by imputing values.

> **Note**: In statistics, imputation is the process of replacing missing data with substituted values. You can learn more about imputations [here](https://en.wikipedia.org/wiki/Imputation_%28statistics%29).

Spikes and dips in telemetry data are temporary anomalies. However, since you are simulating vibration data using sine waves, you can expect a period of "normal" values followed by a high or low value that triggers an anomaly alert. An operator may look for a cluster of anomalies occurring in a short time span, which would signal that something is wrong.

There are other built-in ML models, such as a model for detecting trends. We don't include these models as part of this module, but the student is encouraged to investigate further.

### Visualize data using Power BI

Visualizing numerical data, especially volumes of it, is a challenge in itself. How can we alert a human operator of the sequence of anomalies that infer something is wrong?

The solution we use in this module is to use some built-in functionality of Power BI along with the ability of Azure Stream Analytics to send data in a real-time format that Power BI can ingest.

We use the dashboard feature of Power BI to create a number of tiles. One tile contains the actual vibration measurement. Another tile is a gauge, showing from 0.0 to 1.0 the confidence level that the value is an anomaly. A third tile indicates if the 95% confidence level is reached. Finally, the fourth tile shows the number of anomalies detected over the past hour. By including time as the x-axis, this tile makes it clear if a clutch of anomalies were detected in short succession as they will be clustered together horizontally.

The fourth tile allows you to compare the anomalies with the red text in the telemetry console window. Is there a cluster of anomalies being detected when forced, or increasing, or both, vibrations are in action?

The following resources will be created:

![Lab 8 Architecture](media/LAB_AK_08-architecture.png)

## In This Lab

In this lab, you will complete the following activities:

* Sign-up for Power BI

### Exercise 1: Cold path data processing with Stream Analytics

Power BI can be your personal data analysis and visualization tool, and can also serve as the analytics and decision engine behind group projects, divisions, or entire corporations. Later on in this lab, you will build a dashboard and visualize data using Power BI. This exercise explains how to sign up for Power BI as an individual.


#### Task 1: Add a New Input to the Job

1. From the Azure portal, Navigate to the Resource group and select the pre-created Stream analytics Job Resource.

1. On the **Stream Analytics job** blade, select **Inputs** from the left-hand menu, under **Job Topology**, then select **+Add stream input**, and select **IoT Hub** from the dropdown menu to add an input connected to your IoT Hub.

1. On the **New Input** blade, enter the following:

   - **Input alias**: Enter `temps`
   - Choose **Select IoT Hub from your subscriptions**.
   - **Subscription**: Select the subscription you are using for this hands-on lab.
   - **IoT Hub**: Select the **smartmeter-hub-SUFFIX** IoT Hub.
   - **Endpoint**: Select **Messaging**.
   - **Shared access policy name**: Select **service**.
   - **Consumer Group**: Leave set to **\$Default**.
   - **Event serialization format**: Select **JSON**.
   - **Encoding**: Select **UTF-8**.
   - **Event compression type**: Leave set to **None**.
   
1. Select **Save**.

#### Task 2: Add a New Output to the Job

To create an output, on the left-side menu under **Job topology**, click **Outputs**.

1. On the **Outputs** pane, click **+ Add**, and then click **Blob storage/Data Lake Storage Gen2**.

    The **Blob storage/Data Lake Storage Gen2 - New output** pane is displayed.

1. On the **Blob storage/Data Lake Storage Gen2 - New output** pane, under **Output alias**, enter `vibrationOutput`.

1. Ensure that **Select storage from your subscriptions** is selected.

1. Under **Subscription**, select the subscription you are using for this lab.

1. Under **Storage account**, click **vibrationstore{your-id}**.

    > **Note**:  The **Storage account key** is automatically populated and read-only.

1. Under **Container**, ensure that **Use existing** is selected and that **vibrationcontainer** is selected from the dropdown list.

1. Leave the **Path pattern** blank.

1. Leave the **Date format** and **Time format** at their defaults.

1. Under **Event serialization format**, ensure that **JSON** is selected.

1. Under **Encoding**, ensure that **UTF-8** is selected.

1. Under **Format**, ensure that **Line separated** is selected.

    > **Note**:  This setting stores each record as a JSON object on each line and, taken as a whole, results in a file that is an invalid JSON record. The other option, **Array**, ensures that the entire document is formatted as a JSON array where each record is an item in the array. This allows the entire file to be parsed as valid JSON.

1. Leave **Minimum rows** blank.

1. Under **Maximum time**, leave **Hours** and **Minutes** blank.

1. Under **Authentication mode**, ensure that **Connection string** is selected.

1. To create the output, click **Save**, and then wait for the output to be created.

    The **Outputs** list will be updated with the new output.

1. To edit the query, on the left-side menu under **Job topology**, click **Query**.

1. In the query editor pane, replace the existing query with the query below:

    ```sql
    SELECT
        *
    INTO
        blobs
    FROM
        temperatureinput
    ```

1. Directly above the query editor pane, click **Save Query**.

1. On the left-side menu, click **Overview**.

### Task 3: Check data in blob storage (Storage Explorer)

1. On the left-side menu, click **Storage Explorer (preview)**.

    You can use Storage Explorer for additional reassurance that all of your data is getting to the storage account. 

    > **Note**:  The Storage Explorer is currently in preview mode, so its exact mode of operation may change.

1. In **Storage Explorer (preview)**, under **BLOB CONTAINERS**, click **vibrationcontainer**.

    To view the data, you will need to navigate down a hierarchy of folders. The first folder will be named for the IoT Hub, the next will be a partition, then year, month, day and finally hour. 

1. In the right-hand pane, under **Name**, double-click the folder for your IoT hub, and then use double-clicks to navigate down into the hierarchy until you open the most recent hour folder.

    Within the hour folder, you will see files named for the minute they were generated. This verifies that your data is reaching the storage location as intended.

> **IMPORTANT**: Do not remove these resources until you have completed the Data Visualization module of this course.

### Exercise 2: Hot path data processing with Stream Analytics

Power BI can be your personal data analysis and visualization tool, and can also serve as the analytics and decision engine behind group projects, divisions, or entire corporations. Later on in this lab, you will build a dashboard and visualize data using Power BI. This exercise explains how to sign up for Power BI as an individual.

>**Note:** If you already have a Power BI subscription and you are able to use it during this course, you can skip to Exercise 2.

#### Task 1: Sign up for a Power BI Account

Follow these steps to sign up for a Power BI account. Once you complete this process you will have a Power BI (free) license which you can use to try Power BI on your own using My Workspace, consume content from a Power BI workspace assigned to a Power BI Premium capacity or initiate an individual Power BI Pro Trial.

1. Sign in to your Power BI subscription (https://app.powerbi.com) to see if data is being collected.
 
> **Note**:Connect with the Azure Credentials from Environment Details tab. 

Now you have access to Power BI, you are ready to route real-time telemetry data to a Power BI dashboard.

### Exercise 2: Hot path data processing with Stream Analytics

With this new IoT Hub route in place, and the telemetry data streaming into the Event Hub, you now need to update our Stream Analytics job. This job will need to consume the data from the Event Hub, perform analysis using the **AnomalyDetection_SpikeAndDip** ML model and then output the results to Power BI.

#### Task 1: Add a New Output

1. On the left-side menu under **Job topology**, click **Outputs**.
1. Next, select **Outputs** from the left-hand menu, under **Job Topology**, and select **+ Add**, then select **Power BI** from the drop-down menu
1. In the **Power BI** blade, select **Authorize** to authorize the connection to your Power BI account. When prompted in the popup window, enter the account credentials you used to create your Power BI account in [Before the hands-on lab setup guide, Task 1](./Before%20the%20HOL%20-%20Internet%20of%20Things.md).
1. Once authorized, enter the following:

    - **Output alias**: Set to `powerbi`
    - For the remaining Power BI settings, enter the following:
      - **Group Workspace**: Select the default, **My Workspace**.
      - **Dataset Name**: Enter `avgtemps`
      - **Table Name**: Enter `avgtemps`
      - **Authentication mode**: Select **User token**

1. Select **Save**.

1. Next, select **Query** from the left-hand menu, under **Job Topology**.
1. In the **Query** text box, paste the following query.

    ```sql
    SELECT AVG(temp) AS Average, id
    INTO powerbi
    FROM temps
    GROUP BY TumblingWindow(minute, 5), id
    ```

1. Select **Save query**.
1. Return to the **Overview** blade on your **Stream Analytics job** and select **Start**.
1. In the **Start job** blade, select **Now** (the job will start processing messages from the current point in time onward).
1. Select **Start**.
1. Allow your Stream Analytics Job a few minutes to start.

1. Once the Stream Analytics Job has successfully started, verify that you are showing a non-zero amount of **Input Events** on the **Monitoring** chart on the **Overview** blade. You may need to reconnect your devices on the **Smart Meter Simulator** and let it run for a while to see the events.

### Task 2: Visualize hot data with Power BI

1. Sign in to your Power BI subscription (<https://app.powerbi.com>) to see if data is being collected.

2. Select **My Workspace** on the left-hand menu, then select the **Datasets tab**, and locate the **avgtemps** dataset from the list.

   > **Note:** Sometimes it takes few minutes for the dataset to appear in the Power BI Dataset tab under **My Workspace**

3. Select the **Create Report** button under the **Actions** column.
4. On the **Visualizations** palette, select **Stacked column chart** to create a chart visualization.
5. In the **Fields** listing, drag the **id** field, and drop it into the **Axis** field.
6. Next, drag the **average** field and drop it into the **Value** field.
7. Now, set the **Value** to **Max of average**, by selecting the down arrow next to **average**, and select **Maximum**.
8. Repeat steps 5-8, this time adding a Stacked Column Chart for **Min of average**. (You may need to select on any area of white space on the report designer surface to deselect the Max of average by id chart visualization.)  
9. Next, add a **table visualization**.
10. Set the values to **id** and **Average of average**, by dragging and dropping both fields in the **Values** field, then selecting the dropdown next to **average**, and selecting **Average**.
11. Save the report.
12. Enter the name `Average Temperatures`, and select **Save**.
13. Within the report, select one of the columns to see the data for just that device.

### Exercise 3: View Time Series Insights Explorer 

In this exercise, you will be introduced to working with time series data using the Time Series Insights (TSI) Explorer.

1. If needed, log in to [portal.azure.com](https://portal.azure.com) using your Azure account credentials.

    If you have more than one Azure account, be sure that you are logged in with the account that is tied to the subscription that you will be using for this course.

1. On your Resource group tile, click **tsi-az220-training**.

1. On the **Time Series Insights environment** blade, at the top of the **Overview** pane, click **Go to Environment**.

    This will open the **Time Series Insights Explorer** in a new browser tab.

1. On the toolbar at the top of the page, if there is an option to enable the **Preview**, set **Preview** to **On**.

1. On the left-side menu, ensure that **Analyze** is selected.

    You can expand the navigation menu to display the button names. The two options are "Analyze" and "Model". Choose Analyze.

    Collapse the navigation menu to ensure that you can see the query edit area on the left side of the page.

1. In the query editor, open the **MEASURE** dropdown, and then click **temperature**.

1. Open the **SPLIT BY** dropdown, and then click **iothub-connection-device-id**.

    When you run the query, this will split the graph to show the telemetry from each of the IoT Devices separately on the graph.

1. Click **Add**.

1. At the top of the page, to have the display automatically refresh, click **Auto Refresh**.

    When **Auto refresh** is enabled, the display will be updated every _30 seconds_ to display the latest data. This only applies to the last 1 hour of available data.

1. Notice the graph now displays the **temperature** sensor event data from the IoT Devices within Azure IoT Hub in a _Line Chart_.

1. Notice the list of **Device IDs** to the left of the graph. 

    Hovering the mouse over a specific Device ID will highlight it's data on the graph display.

1. Take a moment to examine the temperature data (graphs) for the telemetry streaming into the system from the three simulated devices.

1. Notice that the spikes in **temperature** of the **sensor-th-container0001** correlate with the temperature spikes of either the **sensor-th-truck0001** or the **sensor-th-airplane0001**.

    This gives you an indication that the sensor-th-container0001 is being transported by Truck or Airplane at those times.

1. To add a second query to the display, set the **MEASURE** dropdown to **humidity**, set the **SPLIT BY** dropdown to **iothub-connection-device-id**, and then click **Add**.

    Notice that there are now two graphs displayed. The top graph shows **temperature** while the lower graph shows **humidity**, both using their own Y-axis scale.

1. Position your mouse pointer over one of the graph lines.

    Notice that when you hover the mouse cursor over the graph, a popup will display the details for a point on the graph. The popup displays the minimum (**min**), average (**avg**), and maximum (**max**) values for the data points in the graph (over the short time represented by that point). The time range associated with the selected data point is displayed along the time axis at the bottom of the display.

1. Above the vertical axis line, notice the options that you can use to control graph settings.

1. Change the Interval setting to 15 seconds.

    Notice how the appearance of the data changes as you increase the interval.

Once you have completed exploring the data, don't forget to stop the container simulator app by pressing **CTRL+C** in the terminal.

